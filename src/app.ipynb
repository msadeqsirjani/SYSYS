{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\sq-pc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.3)\n",
      "Requirement already satisfied: six in c:\\users\\sq-pc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string  as st\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentId</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>.I 1\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>.T\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>18 Editions of the Dewey Decimal Classificatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>.A\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Comaromi, J.P.\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DocumentId                                            Message\n",
       "0          1                                             .I 1\\n\n",
       "1          2                                               .T\\n\n",
       "2          3  18 Editions of the Dewey Decimal Classificatio...\n",
       "3          4                                               .A\\n\n",
       "4          5                                   Comaromi, J.P.\\n"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data. Here it is already in .ALL format.\n",
    "\n",
    "data = {\n",
    "    \"DocumentId\": [],\n",
    "    \"Message\": []\n",
    "}\n",
    "\n",
    "with open('../data/CISI.ALL', encoding='utf-8') as document:\n",
    "    for i, line in enumerate(document):\n",
    "        data[\"DocumentId\"].append(str(i + 1))\n",
    "        data['Message'].append(line)\n",
    "\n",
    "data_frame = pd.DataFrame(data)\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108747, 2)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text cleaning and processing steps-\n",
    "* Remove punctuations\n",
    "* Convert text to tokens\n",
    "* Remove tokens of length less than or equal to 3\n",
    "* Remove stopwords using NLTK corpus stopwords list to match\n",
    "* Apply stemming\n",
    "* Apply lemmatization\n",
    "* Convert words to feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    ''' Remove all punctuations from the text '''\n",
    "    return (\"\".join([ch for ch in text if ch not in st.punctuation]))\n",
    "\n",
    "def tokenize(text):\n",
    "    ''' Convert text to lower case tokens. Here, split() is applied on white-spaces. But, it could be applied\n",
    "        on special characters, tabs or any other string based on which text is to be separated into tokens.\n",
    "    '''\n",
    "    text = re.split('\\s+' ,text)\n",
    "    return (\"\".join([x.lower() for x in text]))\n",
    "\n",
    "def remove_small_words(text):\n",
    "    '''\n",
    "        Remove tokens of length less than 3\n",
    "    '''\n",
    "    return (\"\".join([x for x in text if len(x) > 3]))\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    ''' Remove stopwords. Here, NLTK corpus list is used for a match. However, a customized user-defined \n",
    "        list could be created and used to limit the matches in input text. \n",
    "    '''\n",
    "    return (\"\".join([word for word in text if word not in stopwords]))\n",
    "\n",
    "\n",
    "# Apply stemming to convert tokens to their root form. This is a rule-based process of word form conversion \n",
    "# where word-suffixes are truncated irrespective of whether the root word is an actual word in the language dictionary.\n",
    "# Note that this step is optional and depends on problem type.\n",
    "def stemming(text):\n",
    "    '''\n",
    "        Apply stemming to get root words \n",
    "    '''\n",
    "    ps = PorterStemmer()\n",
    "    return (\"\".join([ps.stem(word) for word in text]))\n",
    "\n",
    "# Lemmatization converts word to it's dictionary base form. This process takes language grammar and vocabulary \n",
    "# into consideration while conversion. Hence, it is different from Stemming in that it does not merely truncate the suffixes \n",
    "# to get the root word.\n",
    "def lemmatize(text):\n",
    "    '''\n",
    "        Apply lemmatization on tokens\n",
    "    '''\n",
    "    word_net = WordNetLemmatizer()\n",
    "    return (\"\".join([word_net.lemmatize(word) for word in text]))\n",
    "\n",
    "def preprocess_pipeline(\n",
    "    df,\n",
    "    tokenize_flag=True,\n",
    "    remove_punctuations_flag=False,\n",
    "    remove_stop_words_flag=False,\n",
    "    remove_small_words_flag=False,\n",
    "    lemmatize_flag=False,\n",
    "    stemmer_flag=False\n",
    "):\n",
    "    \"\"\"\n",
    "    input text \n",
    "        ↳ [tokenize]\n",
    "            ↳ [remove punctuations]  \n",
    "                ↳ [remove stop words]\n",
    "                    ↳ [remove small words]\n",
    "                        ↳ [lemmatize]\n",
    "                            ↳ [stemmer]\n",
    "                                ↳ output text\n",
    "    \"\"\"\n",
    "    df['PreProcessed'] = df['Message']\n",
    "\n",
    "    if(tokenize_flag):\n",
    "        df['PreProcessed'] = df['PreProcessed'].apply(lambda x: tokenize(x))\n",
    "\n",
    "    if remove_punctuations_flag:\n",
    "        df['PreProcessed'] = df['PreProcessed'].apply(lambda x: remove_punctuations(x))\n",
    "\n",
    "    if remove_stop_words_flag:\n",
    "        df['PreProcessed'] = df['PreProcessed'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "    if remove_small_words_flag:\n",
    "        df['PreProcessed'] = df['PreProcessed'].apply(lambda x: remove_small_words(x))            \n",
    "\n",
    "    if lemmatize_flag:\n",
    "        df['PreProcessed'] = df['PreProcessed'].apply(lambda x: lemmatize(x))\n",
    "\n",
    "    if stemmer_flag:\n",
    "        df['PreProcessed'] = df['PreProcessed'].apply(lambda x: stemming(x))            \n",
    "\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentId</th>\n",
       "      <th>Message</th>\n",
       "      <th>PreProcessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>.I 1\\n</td>\n",
       "      <td>i1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>.T\\n</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>18 Editions of the Dewey Decimal Classificatio...</td>\n",
       "      <td>18editionsofthedeweydecimalclassifications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>.A\\n</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Comaromi, J.P.\\n</td>\n",
       "      <td>comaromijp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DocumentId                                            Message  \\\n",
       "0          1                                             .I 1\\n   \n",
       "1          2                                               .T\\n   \n",
       "2          3  18 Editions of the Dewey Decimal Classificatio...   \n",
       "3          4                                               .A\\n   \n",
       "4          5                                   Comaromi, J.P.\\n   \n",
       "\n",
       "                                 PreProcessed  \n",
       "0                                          i1  \n",
       "1                                           t  \n",
       "2  18editionsofthedeweydecimalclassifications  \n",
       "3                                           a  \n",
       "4                                  comaromijp  "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_pipeline(df=data_frame, \n",
    "                    tokenize_flag=True, \n",
    "                    remove_punctuations_flag=True, \n",
    "                    remove_small_words_flag=False,\n",
    "                    remove_stop_words_flag=False,\n",
    "                    lemmatize_flag=True,\n",
    "                    stemmer_flag=True)\n",
    "\n",
    "data_frame.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f049f70bb1cc903f2a8aa743f371e4512dfa343431baa315c1f370fd7c1e81b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
